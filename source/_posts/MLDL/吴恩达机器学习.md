---
title: 吴恩达机器学习
date: 2019-01-24 09:27:27
categories: MLDL
tags: [Note,Mechine learning]
---
笔记。

<!---more--->

# 绪论

## 1 Welcome

## 2 Definition

Definition：A computer program is said to *learn frome experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.

- Supervised learning
- Unsupervised learning

## 3 Supervised learning

监督学习

Regression(回归): Predict continuous valued output 目标是一个连续值输出

分类：目标离散

## 4 Unsupervised learning

无监督学习。

事先不知道答案。聚类。

# Univariate linear regression(Linear regression with one variable)

 单变量线性回归

## 6

代价函数 J()

## 7 8 9 Cost Function 代价函数

## 10 11Gradient descent

可以将代价函数最小化的梯度下降法。

- 随意设置起始点，假设你站在一座山上，环顾四周，看最快的下山路径是往哪，走一步，再继续看往哪走。直到走到局部最优。

- 起点不同，局部最优不同

α决定迈步子的大小，大则梯度下降迅速。

## 12 Gradient descent for linear regression



#线代 

## 14~19 线代

# 配置

## 20~27

环境安装

# 多变量线性回归

## 28 多特性

## 29 多元梯度下降

http://mobile.yangkeduo.com/coup ... 06573083_JmqeZYhLWBhttp://mobile.yangkeduo.com/coup ... 06573083_JmqeZYhLWB

## 30 技巧：特征缩放

参数的规模不同导致几个θ产生椭圆等高线。这样寻找局部最优的方法变得缓慢。

特征缩放，把房子的面积➗100. 让几个特征规模相似。 产生圆形特征图

特征缩放不需要太精确，只是为了让收敛所需的迭代次数减少。

## 31 学习率α

代价函数随迭代次数++反而越来越大，说明学习率该设的小一点。

太小：收敛的太慢

太大：可能无法收敛（错过最优）

## 32 多项式回归

## 33 正规方程

给了一个公式直接能求出 θ 这个向量的值，就是一个个系数。

5分41秒

`pinv(X'*X)*X'*y`

- 比较梯度下降法和正规方程m个example，n个特性：

| Gradient Descent    | Normal Equation                                  |
| ------------------- | ------------------------------------------------ |
| 需选择α             | 不需要                                           |
| 需要迭代            | 不需要                                           |
| But                 |                                                  |
| n很大还是能很好工作 | O(N3)<br />如果n是1000级别，还能用。再大就不行了 |

## 34 正规方程的矩阵不可逆情况

不可逆矩阵X，在Octave里输入

```octave
pinv(X'*X)*X'*y
```

也是可以得到正确答案的。

---

造成X不可逆的情况，可能是特征Xm和Xn有线性关系。可以去掉一些特征。

# Octave/Matlab教程

向量化

# Logistic 回归

## 46

虽然名字里有回归，但这是一个分类算法。

这一节仅介绍引入。

## 50 

代价函数。

求解θ同样可以用梯度下降法，并用特征算法来加快收敛速度。

## 51 高级优化

Conjugate gradient

BFGS 

L-BFGS

超出本课范畴

在复杂、大规模的数据里使用这些算法

## 52 多元化

两两地进行分割。

最后再分别带入分类器根据可能性选择。

# 正则化

## 55过拟合问题

underfitting 欠拟合 高度差

overfitting 过拟合  高方差：项太多了  为了完全地去模仿拟合训练集，导致参数很多，模型变差了 -->

- 舍弃一些变量。
- 正则化

# 神经网络

解决特征n特别大的情况。如一幅图的像素点全是特征。

